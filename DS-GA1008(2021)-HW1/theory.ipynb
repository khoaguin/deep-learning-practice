{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 1: Backpropagation\n",
    "## Deep Learning (NYU) 2021\n",
    "Link: https://atcold.github.io/NYU-DLSP21/\n",
    "Following is my answers for the theory part of the homework, they are not verified to be correct. Take it with a big grain of salt.\n",
    "### Theory\n",
    "#### Instructions:\n",
    "1. Every vector is treated as a column vector\n",
    "2. Use [numerator-layout notation](https://en.wikipedia.org/wiki/Matrix_calculus#Numerator-layout_notation) for matrix calculus. \n",
    "3. Only use vector and matrix (no tensor)\n",
    "4. Missing transpose are wrong.\n",
    "\n",
    "#### 1.1 Two-Layer Neural Nets\n",
    "We have the following neural net:  \n",
    "$$\\boldsymbol{x} → \\text{Linear}_1 → f → \\text{Linear}_2 → g → \\boldsymbol{\\hat{y}}, $$\n",
    "where $\\text{Linear}_i(x) = \\boldsymbol{W^{(i)}}\\boldsymbol{x} + \\boldsymbol{b^{(i)}}$, and $f, g$ are element-wise nonlinear activation functions. $\\boldsymbol{x} \\in \\mathbb{R}^n$, $\\boldsymbol{\\hat{y}} \\in \\mathbb{R}^K$.\n",
    "\n",
    "#### 1.2 Regression Task\n",
    "Choose $f = \\text{ReLU}$, and $g$ to be an identity function. We choose MSE as the loss function: $l_{MSE}(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\Vert \\boldsymbol{y} - \\boldsymbol{\\hat{y}} \\Vert^2$\n",
    "1. **Name and mathematically describe the 5 programming steps you would take to train this model with `PyTorch` using SGD on a single batch of data.**\n",
    "* Step 1: set all the gradients to zeros as \n",
    "$$\\frac{\\partial l_{MSE}}{\\partial\\boldsymbol{W^{(i)}}} = 0, \\frac{\\partial l_{MSE}}{\\partial\\boldsymbol{b^{(i)}}} = 0$$\n",
    "* Step 2: do the forward pass, put $\\boldsymbol{x}$ through the network and get $\\boldsymbol{\\hat{y}}$\n",
    "* Step 3: calculate the loss $l_{MSE}(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\Vert \\boldsymbol{y} - \\boldsymbol{\\hat{y}} \\Vert^2$\n",
    "* Step 4: calculate the gradients of the loss function with respect to the weights $$\\frac{\\partial l_{MSE}}{\\partial\\boldsymbol{W^{(i)}}}, \\frac{\\partial l_{MSE}}{\\partial\\boldsymbol{b^{(i)}}}$$\n",
    "* Step 5: update the weights according to the calculated gradients as\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{W^{(i)}} ← \\boldsymbol{W^{(i)}}-\\gamma\\frac{\\partial l_{MSE}}{\\partial\\boldsymbol{W^{(i)}}}, \\\\  \\boldsymbol{b^{(i)}} ← \\boldsymbol{b^{(i)}}-\\gamma\\frac{\\partial l_{MSE}}{\\partial\\boldsymbol{b^{(i)}}}\n",
    "\\end{align}\n",
    "$$\n",
    "2. **For a single data point $(\\boldsymbol{x}, \\boldsymbol{y})$, write down all inputs and outputs for forward pass of each layer. You can only use variable $\\boldsymbol{x}, \\boldsymbol{y}, \\boldsymbol{W}^{(1)}, \\boldsymbol{b}^{(1)}, \\boldsymbol{W}^{(2)}, \\boldsymbol{b}^{(2)}$ in your answer (note that $\\text{Linear}_i(x) = \\boldsymbol{W^{(i)}}\\boldsymbol{x} + \\boldsymbol{b^{(i)}}$).**\n",
    "\n",
    "| Layer                 | Input                | Output        |\n",
    "| :---                  |    :----:            |          ---: |\n",
    "| $\\text{Linear}_1$     | $\\boldsymbol{x}$     |  $\\boldsymbol{z}_1 = \\boldsymbol{W^{(1)}}\\boldsymbol{x} + \\boldsymbol{b^{(1)}}$  |\n",
    "| $f$                   | $\\boldsymbol{z}_1$        | $\\boldsymbol{z}_2 = \\text{ReLU}(\\boldsymbol{z}_1)$     |\n",
    "| $\\text{Linear}_2$     | $\\boldsymbol{z}_2$      | $\\boldsymbol{z}_3 = \\boldsymbol{W^{(2)}}\\boldsymbol{z}_2 + \\boldsymbol{b^{(2)}}$   |\n",
    "| $g$                   | $\\boldsymbol{z}_3$        |   $\\boldsymbol{\\hat{y}} = \\boldsymbol{z}_3$    |\n",
    "| Loss                  | $\\boldsymbol{\\hat{y}}, \\boldsymbol{y}$       | $\\Vert \\boldsymbol{y} - \\boldsymbol{\\hat{y}} \\Vert^2$ |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. **Write down the gradient calculated from the backward pass. You can only use the following variables: $\\boldsymbol{x}, \\boldsymbol{y}, \\boldsymbol{W}^{(1)}, \\boldsymbol{b}^{(1)}, \\boldsymbol{W}^{(2)}, \\boldsymbol{b}^{(2)},\\frac{∂l}{∂\\boldsymbol{\\hat{y}}},\\frac{∂\\boldsymbol{z}_2}{∂\\boldsymbol{z}_1}, \\frac{∂\\boldsymbol{\\hat{y}}}{∂\\boldsymbol{z}_3}$ in your answer.**\n",
    "\n",
    "| Parameter                 |   Gradient           |\n",
    "| :---                      |    :----:            |\n",
    "| $\\boldsymbol{W^{(1)}}$    |                      |    \n",
    "| $\\boldsymbol{b^{(1)}}$    |                      |  \n",
    "| $\\boldsymbol{W^{(2)}}$    |   $\\frac{∂l}{∂\\boldsymbol{\\hat{y}}} * \\frac{∂\\boldsymbol{\\hat{y}}}{∂\\boldsymbol{z}_3}* \\frac{∂\\boldsymbol{z}_3}{∂\\boldsymbol{z}1}=\\frac{∂l}{∂\\boldsymbol{\\hat{y}}} * \\frac{∂\\boldsymbol{\\hat{y}}}{∂\\boldsymbol{z}_3}* \\boldsymbol{z}_2$                   |    \n",
    "| $\\boldsymbol{b^{(2)}}$    |   $\\frac{∂l}{∂\\boldsymbol{\\hat{y}}} * \\frac{∂\\boldsymbol{\\hat{y}}}{∂\\boldsymbol{z}_3}$                   |     "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}